{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "###############################################################################\n",
    "##Defining the necessities\n",
    "#Defining (n, k)\n",
    "num_channels = 2\n",
    "num_messages = 16\n",
    "num_bits = 2\n",
    "###############################################################################\n",
    "##Generating the training data, one hot vectors of width num_messages\n",
    "num_samples = 10000\n",
    "training_data = np.zeros([num_samples, num_messages])\n",
    "\n",
    "file = open(\"training_data.npy\", 'wb')\n",
    "\n",
    "for iter in range(num_samples):\n",
    "    training_data[iter, int(iter/(int((num_samples+1)/num_messages)))] = 1\n",
    "\n",
    "np.save(file, training_data)\n",
    "print(training_data)\n",
    "file.close()\n",
    "###############################################################################\n",
    "##Generating the test data, one hot vectors of width num_messages\n",
    "num_samples = 5000\n",
    "test_data = np.zeros([num_samples, num_messages])\n",
    "\n",
    "file = open(\"test_data.npy\", 'wb')\n",
    "\n",
    "for iter in range(num_samples):\n",
    "    test_data[iter, np.random.randint(num_messages)] = 1\n",
    "\n",
    "np.save(file, test_data)\n",
    "print(test_data)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midlayer_dim : 10\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 1s 124us/step - loss: 2.3331 - val_loss: 1.8684\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 1.5904 - val_loss: 1.2381\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 1.1734 - val_loss: 0.9199\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 0.9074 - val_loss: 0.7054\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.7199 - val_loss: 0.5245\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 0s 47us/step - loss: 0.5842 - val_loss: 0.3999\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 0.4742 - val_loss: 0.2924\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.3998 - val_loss: 0.2159\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.3329 - val_loss: 0.1743\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 0.2726 - val_loss: 0.1243\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 0.2395 - val_loss: 0.0998\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.2209 - val_loss: 0.0803\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 0.1838 - val_loss: 0.0621\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 0.1610 - val_loss: 0.0513\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.1586 - val_loss: 0.0449\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 0.1323 - val_loss: 0.0358\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 0.1338 - val_loss: 0.0311\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.1341 - val_loss: 0.0283\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.1012 - val_loss: 0.0226\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 0.1046 - val_loss: 0.0178\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.0926 - val_loss: 0.0177\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0941 - val_loss: 0.0157\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 0.1017 - val_loss: 0.0157\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.0844 - val_loss: 0.0109\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0873 - val_loss: 0.0121\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 0.0781 - val_loss: 0.0093\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.0726 - val_loss: 0.0078\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.0592 - val_loss: 0.0065\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.0509 - val_loss: 0.0059\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 0.0481 - val_loss: 0.0058\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.0648 - val_loss: 0.0052\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.0671 - val_loss: 0.0049\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 0.0574 - val_loss: 0.0044\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 0.0545 - val_loss: 0.0045\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 0.0625 - val_loss: 0.0039\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.0655 - val_loss: 0.0037\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 0.0528 - val_loss: 0.0038\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0419 - val_loss: 0.0033\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 0.0378 - val_loss: 0.0027\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 0.0560 - val_loss: 0.0025\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 0.0463 - val_loss: 0.0032\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.0719 - val_loss: 0.0026\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0494 - val_loss: 0.0024\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0487 - val_loss: 0.0021\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 0.0402 - val_loss: 0.0021\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0586 - val_loss: 0.0027\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0495 - val_loss: 0.0022\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0414 - val_loss: 0.0017\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 0.0446 - val_loss: 0.0020\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 0.0480 - val_loss: 0.0025\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 0.0533 - val_loss: 0.0014\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 0.0520 - val_loss: 0.0022\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 0.0320 - val_loss: 0.0016\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 0.0300 - val_loss: 0.0015\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 0.0426 - val_loss: 0.0010\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.0553 - val_loss: 0.0017\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0358 - val_loss: 0.0011\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 0.0360 - val_loss: 0.0010\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 0.0284 - val_loss: 0.0011\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 0.0300 - val_loss: 0.0011\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 1s 55us/step - loss: 0.0542 - val_loss: 0.0011\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.0374 - val_loss: 8.5353e-04\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 0.0459 - val_loss: 0.0014\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 1s 59us/step - loss: 0.0462 - val_loss: 8.3797e-04\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 0s 47us/step - loss: 0.0508 - val_loss: 7.0356e-04\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 1s 51us/step - loss: 0.0208 - val_loss: 6.8715e-04\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.0429 - val_loss: 8.0786e-04\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 0.0289 - val_loss: 7.6136e-04\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 0.0325 - val_loss: 6.5287e-04\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.0488 - val_loss: 8.2120e-04\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 0.0242 - val_loss: 5.9827e-04\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 0s 47us/step - loss: 0.0236 - val_loss: 6.5342e-04\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0307 - val_loss: 5.7831e-04\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0250 - val_loss: 4.8189e-04\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0217 - val_loss: 6.8058e-04\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.0361 - val_loss: 5.2575e-04\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0271 - val_loss: 5.1089e-04\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0359 - val_loss: 6.2890e-04\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 0.0287 - val_loss: 5.0116e-04\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0337 - val_loss: 5.3508e-04\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0194 - val_loss: 5.9984e-04\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0210 - val_loss: 4.3009e-04\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0557 - val_loss: 8.1899e-04\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0253 - val_loss: 4.1257e-04\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0471 - val_loss: 7.6122e-04\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 0.0386 - val_loss: 4.5378e-04\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0495 - val_loss: 4.5886e-04\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0222 - val_loss: 4.1103e-04\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.0437 - val_loss: 4.6320e-04\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.0349 - val_loss: 4.4556e-04\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0250 - val_loss: 5.0732e-04\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0250 - val_loss: 4.9283e-04\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.0252 - val_loss: 2.8826e-04\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0174 - val_loss: 3.5147e-04\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0304 - val_loss: 3.7666e-04\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0269 - val_loss: 3.3177e-04\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.0314 - val_loss: 4.5453e-04\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0291 - val_loss: 2.8064e-04\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 0s 34us/step - loss: 0.0676 - val_loss: 5.6656e-04\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 0.0312 - val_loss: 3.1431e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###############################################################################\n",
    "file_train = open(\"training_data.npy\", \"rb\")\n",
    "file_test = open(\"test_data.npy\", \"rb\")\n",
    "\n",
    "training_data = np.load(file_train)\n",
    "test_data = np.load(file_test)\n",
    "\n",
    "file_train.close()\n",
    "file_test.close()\n",
    "###############################################################################\n",
    "learning_rate = 0.001\n",
    "input_dim = 16\n",
    "encoding_dim = 2\n",
    "midlayer_dim = int((input_dim+encoding_dim)/2) +1\n",
    "print (\"midlayer_dim :\", midlayer_dim )\n",
    "energy_per_bit = 5\n",
    "\n",
    "input_msg = Input(shape = (input_dim, ))\n",
    "\n",
    "\n",
    "encoded = Dense(input_dim, activation='relu')(input_msg)\n",
    "encoded2 = Dense(encoding_dim, activation = 'linear')(encoded)\n",
    "#encoded3 = keras.layers.Lambda(lambda x: keras.backend.l2_normalize(x, axis=0))(encoded2)\n",
    "#encoded3 = keras.layers.Lambda(lambda x:np.sqrt(encoding_dim)*keras.backend.l2_normalize(x, axis=1))(encoded2)\n",
    "encoded3 = keras.layers.BatchNormalization(axis=1)(encoded2)\n",
    "encoded4 = keras.layers.GaussianNoise(np.sqrt(encoding_dim/(2*input_dim*energy_per_bit)))(encoded3)\n",
    "\n",
    "decoded3 = Dense(midlayer_dim, activation = 'relu')(encoded4)\n",
    "decoded  = Dense(input_dim, activation = \"softmax\") (decoded3)\n",
    "\n",
    "adam = Adam(learning_rate)\n",
    "sgd = SGD(learning_rate)\n",
    "\n",
    "autoencoder = Model(input_msg, decoded)\n",
    "autoencoder.compile(optimizer = adam, loss='categorical_crossentropy')\n",
    "\n",
    "###############################################################################\n",
    "##Preparing the encoder and the decoder\n",
    "encoder = Model(input_msg, encoded3)\n",
    "\n",
    "encoded_input = Input(shape=(encoding_dim, ))\n",
    "decoder_layer1 = autoencoder.layers[-2](encoded_input)\n",
    "decoder_layer2 = autoencoder.layers[-1](decoder_layer1)\n",
    "#decoder_layer3 = autoencoder.layers[-1](decoder_layer2)\n",
    "\n",
    "decoder = Model(encoded_input, decoder_layer2)\n",
    "###############################################################################\n",
    "\n",
    "autoencoder.fit(training_data, training_data, epochs=100, batch_size=50, shuffle=True, validation_data=(test_data, test_data))\n",
    "###############################################################################\n",
    "test_predictions = encoder.predict(test_data)\n",
    "\n",
    "plt.scatter(test_predictions[:, 0], test_predictions[:, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
