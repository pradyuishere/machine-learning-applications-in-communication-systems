{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n",
      "[[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "###############################################################################\n",
    "##Defining the necessities\n",
    "#Defining (n, k)\n",
    "num_channels = 2\n",
    "num_messages = 4\n",
    "num_bits = 2\n",
    "################################################################################\n",
    "##Generating the training data, one hot vectors of width num_messages\n",
    "num_samples = 10000\n",
    "training_data = np.zeros([num_samples, num_messages])\n",
    "\n",
    "file = open(\"training_data-a.npy\", 'wb')\n",
    "file1 = open(\"training_data-b.npy\", 'wb')\n",
    "\n",
    "for iter in range(num_samples):\n",
    "\n",
    "    training_data[iter, np.random.randint(num_messages)] = 1\n",
    "np.save(file, training_data)\n",
    "print(training_data)\n",
    "\n",
    "training_data = np.zeros([num_samples, num_messages])\n",
    "\n",
    "for iter in range(num_samples):\n",
    "    training_data[iter, np.random.randint(num_messages)] = 1\n",
    "\n",
    "np.save(file1, training_data)\n",
    "print(training_data)\n",
    "file.close()\n",
    "file1.close()\n",
    "###############################################################################\n",
    "##Generating the test data, one hot vectors of width num_messages\n",
    "num_samples = 5000\n",
    "test_data = np.zeros([num_samples, num_messages])\n",
    "\n",
    "file = open(\"test_data-a.npy\", 'wb')\n",
    "file1 = open(\"test_data-b.npy\", 'wb')\n",
    "\n",
    "for iter in range(num_samples):\n",
    "    test_data[iter, np.random.randint(num_messages)] = 1\n",
    "\n",
    "np.save(file, test_data)\n",
    "print(test_data)\n",
    "\n",
    "test_data = np.zeros([num_samples, num_messages])\n",
    "\n",
    "for iter in range(num_samples):\n",
    "    test_data[iter, np.random.randint(num_messages)] = 1\n",
    "\n",
    "np.save(file1, test_data)\n",
    "\n",
    "file.close()\n",
    "file1.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 1. 0.]]\n",
      "(5000, 8)\n"
     ]
    }
   ],
   "source": [
    "file_train = open(\"training_data-a.npy\", \"rb\")\n",
    "file_train1 = open(\"training_data-b.npy\", \"rb\")\n",
    "file_test = open(\"test_data-a.npy\", \"rb\")\n",
    "file_test1 = open(\"test_data-b.npy\", \"rb\")\n",
    "\n",
    "training_data_a = np.load(file_train)\n",
    "test_data_a = np.load(file_test)\n",
    "training_data_b = np.load(file_train1)\n",
    "test_data_b = np.load(file_test1)\n",
    "\n",
    "training_data = np.concatenate((training_data_a, training_data_b), axis = 1)\n",
    "test_data  = np.concatenate((test_data_a, test_data_b), axis = 1)\n",
    "\n",
    "file_train.close()\n",
    "file_test.close()\n",
    "file_train1.close()\n",
    "file_test1.close()\n",
    "\n",
    "print(training_data)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midlayer_dim : 4\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "msg_len = 4\n",
    "input_dim = 2*msg_len\n",
    "channel_num = 2\n",
    "encoding_dim = 2*channel_num\n",
    "midlayer_dim = int((msg_len+channel_num)/2)+1\n",
    "print (\"midlayer_dim :\", midlayer_dim )\n",
    "energy_per_bit1 = 5\n",
    "energy_per_bit2 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape :  8\n"
     ]
    }
   ],
   "source": [
    "input_msg1 = Input(shape = (msg_len, ))\n",
    "print(\"Input shape : \", input_dim)\n",
    "input_msg2 = Input(shape = (msg_len, ))\n",
    "\n",
    "\n",
    "lambda_l1_1 = keras.layers.Lambda(lambda x : x)(input_msg1)\n",
    "encoded_l1_1 = Dense(msg_len, activation = 'relu')(lambda_l1_1)\n",
    "encoded_l1_2 = Dense(channel_num, activation = 'linear')(encoded_l1_1)\n",
    "encoded_l1_3 = keras.layers.BatchNormalization(axis=1)(encoded_l1_2)\n",
    "\n",
    "lambda_l2_1 = keras.layers.Lambda(lambda x : x)(input_msg2)\n",
    "encoded_l2_1 = Dense(msg_len, activation = 'relu')(lambda_l2_1)\n",
    "encoded_l2_2 = Dense(channel_num, activation = 'linear')(encoded_l2_1)\n",
    "encoded_l2_3 = keras.layers.BatchNormalization(axis=1)(encoded_l2_2)\n",
    "\n",
    "added_layer = keras.layers.Add()([encoded_l1_3, encoded_l2_3])\n",
    "encoded_l1_4 = keras.layers.GaussianNoise(np.sqrt(channel_num/(2*msg_len*energy_per_bit1)))(added_layer)\n",
    "encoded_l2_4 = keras.layers.GaussianNoise(np.sqrt(channel_num/(2*msg_len*energy_per_bit2)))(added_layer)\n",
    "\n",
    "decoded_l1_1 = Dense(midlayer_dim, activation = 'relu')(encoded_l1_4)\n",
    "decoded_l2_1 = Dense(midlayer_dim, activation = 'relu')(encoded_l2_4)\n",
    "\n",
    "decoded_l1_2 = Dense(midlayer_dim, activation = 'softmax')(decoded_l1_1)\n",
    "decoded_l2_2 = Dense(midlayer_dim, activation = 'softmax')(decoded_l2_1)\n",
    "\n",
    "# concat_layer = keras.layers.concatenate([decoded_l1_2, decoded_l2_2], axis = 1)\n",
    "\n",
    "# decoded_out = Dense(input_dim)(concat_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(learning_rate)\n",
    "autoencoder = Model([input_msg1, input_msg2], [decoded_l1_2, decoded_l2_2])\n",
    "autoencoder.compile(optimizer = sgd, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder1 = Model(input_msg1, encoded_l1_3)\n",
    "encoder2 = Model(input_msg2, encoded_l2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4)\n",
      "Model: \"model_32\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 4)            0           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 4)            0           input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_61 (Dense)                (None, 4)            20          lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_63 (Dense)                (None, 4)            20          lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_62 (Dense)                (None, 2)            10          dense_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_64 (Dense)                (None, 2)            10          dense_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 2)            8           dense_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 2)            8           dense_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 2)            0           batch_normalization_16[0][0]     \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_16 (GaussianNois (None, 2)            0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_17 (GaussianNois (None, 2)            0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_65 (Dense)                (None, 4)            12          gaussian_noise_16[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_66 (Dense)                (None, 4)            12          gaussian_noise_17[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_67 (Dense)                (None, 4)            20          dense_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_68 (Dense)                (None, 4)            20          dense_66[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 140\n",
      "Trainable params: 132\n",
      "Non-trainable params: 8\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 2s 199us/step - loss: 0.2954 - dense_67_loss: 0.1566 - dense_68_loss: 0.1388 - val_loss: 0.2341 - val_dense_67_loss: 0.1299 - val_dense_68_loss: 0.1042\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 1s 57us/step - loss: 0.2036 - dense_67_loss: 0.1110 - dense_68_loss: 0.0925 - val_loss: 0.1650 - val_dense_67_loss: 0.0908 - val_dense_68_loss: 0.0741\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 0.1647 - dense_67_loss: 0.0871 - dense_68_loss: 0.0775 - val_loss: 0.1340 - val_dense_67_loss: 0.0724 - val_dense_68_loss: 0.0617\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 0.1396 - dense_67_loss: 0.0711 - dense_68_loss: 0.0685 - val_loss: 0.1025 - val_dense_67_loss: 0.0525 - val_dense_68_loss: 0.0500\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 1s 57us/step - loss: 0.1079 - dense_67_loss: 0.0545 - dense_68_loss: 0.0534 - val_loss: 0.0673 - val_dense_67_loss: 0.0345 - val_dense_68_loss: 0.0328\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 1s 51us/step - loss: 0.0760 - dense_67_loss: 0.0387 - dense_68_loss: 0.0373 - val_loss: 0.0426 - val_dense_67_loss: 0.0218 - val_dense_68_loss: 0.0209\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 1s 57us/step - loss: 0.0587 - dense_67_loss: 0.0294 - dense_68_loss: 0.0293 - val_loss: 0.0284 - val_dense_67_loss: 0.0149 - val_dense_68_loss: 0.0136\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 0.0473 - dense_67_loss: 0.0236 - dense_68_loss: 0.0237 - val_loss: 0.0221 - val_dense_67_loss: 0.0124 - val_dense_68_loss: 0.0096\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 1s 57us/step - loss: 0.0416 - dense_67_loss: 0.0183 - dense_68_loss: 0.0233 - val_loss: 0.0170 - val_dense_67_loss: 0.0089 - val_dense_68_loss: 0.0081\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 0.0339 - dense_67_loss: 0.0176 - dense_68_loss: 0.0163 - val_loss: 0.0118 - val_dense_67_loss: 0.0069 - val_dense_68_loss: 0.0049\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.0280 - dense_67_loss: 0.0150 - dense_68_loss: 0.0130 - val_loss: 0.0099 - val_dense_67_loss: 0.0056 - val_dense_68_loss: 0.0043\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.0247 - dense_67_loss: 0.0124 - dense_68_loss: 0.0123 - val_loss: 0.0086 - val_dense_67_loss: 0.0049 - val_dense_68_loss: 0.0037\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 0s 50us/step - loss: 0.0253 - dense_67_loss: 0.0132 - dense_68_loss: 0.0121 - val_loss: 0.0055 - val_dense_67_loss: 0.0030 - val_dense_68_loss: 0.0025\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 1s 57us/step - loss: 0.0209 - dense_67_loss: 0.0101 - dense_68_loss: 0.0108 - val_loss: 0.0064 - val_dense_67_loss: 0.0040 - val_dense_68_loss: 0.0024\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.0212 - dense_67_loss: 0.0100 - dense_68_loss: 0.0111 - val_loss: 0.0049 - val_dense_67_loss: 0.0021 - val_dense_68_loss: 0.0028\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 1s 62us/step - loss: 0.0187 - dense_67_loss: 0.0086 - dense_68_loss: 0.0101 - val_loss: 0.0047 - val_dense_67_loss: 0.0027 - val_dense_68_loss: 0.0020\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 1s 67us/step - loss: 0.0178 - dense_67_loss: 0.0091 - dense_68_loss: 0.0087 - val_loss: 0.0037 - val_dense_67_loss: 0.0018 - val_dense_68_loss: 0.0018\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 1s 71us/step - loss: 0.0135 - dense_67_loss: 0.0063 - dense_68_loss: 0.0072 - val_loss: 0.0034 - val_dense_67_loss: 0.0016 - val_dense_68_loss: 0.0018\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 1s 68us/step - loss: 0.0176 - dense_67_loss: 0.0090 - dense_68_loss: 0.0086 - val_loss: 0.0029 - val_dense_67_loss: 0.0015 - val_dense_68_loss: 0.0014\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 1s 72us/step - loss: 0.0132 - dense_67_loss: 0.0062 - dense_68_loss: 0.0070 - val_loss: 0.0026 - val_dense_67_loss: 0.0012 - val_dense_68_loss: 0.0014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 0.0163 - dense_67_loss: 0.0073 - dense_68_loss: 0.0090 - val_loss: 0.0020 - val_dense_67_loss: 0.0011 - val_dense_68_loss: 9.0217e-04\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 0s 50us/step - loss: 0.0171 - dense_67_loss: 0.0082 - dense_68_loss: 0.0090 - val_loss: 0.0031 - val_dense_67_loss: 0.0021 - val_dense_68_loss: 9.6484e-04\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 1s 56us/step - loss: 0.0144 - dense_67_loss: 0.0073 - dense_68_loss: 0.0071 - val_loss: 0.0029 - val_dense_67_loss: 9.4860e-04 - val_dense_68_loss: 0.0019\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 0s 47us/step - loss: 0.0151 - dense_67_loss: 0.0082 - dense_68_loss: 0.0069 - val_loss: 0.0018 - val_dense_67_loss: 8.4257e-04 - val_dense_68_loss: 9.5716e-04\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.0126 - dense_67_loss: 0.0054 - dense_68_loss: 0.0072 - val_loss: 0.0017 - val_dense_67_loss: 8.0371e-04 - val_dense_68_loss: 8.5484e-04\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 1s 51us/step - loss: 0.0143 - dense_67_loss: 0.0068 - dense_68_loss: 0.0075 - val_loss: 0.0016 - val_dense_67_loss: 6.6159e-04 - val_dense_68_loss: 9.0423e-04\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 1s 55us/step - loss: 0.0136 - dense_67_loss: 0.0072 - dense_68_loss: 0.0064 - val_loss: 0.0013 - val_dense_67_loss: 6.9591e-04 - val_dense_68_loss: 5.9557e-04\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.0103 - dense_67_loss: 0.0051 - dense_68_loss: 0.0052 - val_loss: 0.0016 - val_dense_67_loss: 6.2326e-04 - val_dense_68_loss: 9.9284e-04\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 1s 51us/step - loss: 0.0119 - dense_67_loss: 0.0063 - dense_68_loss: 0.0057 - val_loss: 0.0018 - val_dense_67_loss: 0.0012 - val_dense_68_loss: 5.8070e-04\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 1s 73us/step - loss: 0.0108 - dense_67_loss: 0.0046 - dense_68_loss: 0.0062 - val_loss: 0.0016 - val_dense_67_loss: 8.3212e-04 - val_dense_68_loss: 7.7256e-04\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.0101 - dense_67_loss: 0.0050 - dense_68_loss: 0.0051 - val_loss: 0.0011 - val_dense_67_loss: 5.5631e-04 - val_dense_68_loss: 4.9826e-04\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 0.0097 - dense_67_loss: 0.0055 - dense_68_loss: 0.0043 - val_loss: 9.1027e-04 - val_dense_67_loss: 5.0152e-04 - val_dense_68_loss: 4.0875e-04\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 1s 55us/step - loss: 0.0107 - dense_67_loss: 0.0055 - dense_68_loss: 0.0052 - val_loss: 9.9653e-04 - val_dense_67_loss: 5.5412e-04 - val_dense_68_loss: 4.4241e-04\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.0127 - dense_67_loss: 0.0076 - dense_68_loss: 0.0052 - val_loss: 8.0574e-04 - val_dense_67_loss: 4.7319e-04 - val_dense_68_loss: 3.3255e-04\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 1s 54us/step - loss: 0.0139 - dense_67_loss: 0.0060 - dense_68_loss: 0.0078 - val_loss: 0.0017 - val_dense_67_loss: 8.5135e-04 - val_dense_68_loss: 8.1317e-04\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 1s 54us/step - loss: 0.0075 - dense_67_loss: 0.0032 - dense_68_loss: 0.0043 - val_loss: 9.0124e-04 - val_dense_67_loss: 4.6175e-04 - val_dense_68_loss: 4.3950e-04\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 1s 54us/step - loss: 0.0083 - dense_67_loss: 0.0043 - dense_68_loss: 0.0040 - val_loss: 8.5710e-04 - val_dense_67_loss: 3.8326e-04 - val_dense_68_loss: 4.7385e-04\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 0s 47us/step - loss: 0.0077 - dense_67_loss: 0.0037 - dense_68_loss: 0.0041 - val_loss: 8.1921e-04 - val_dense_67_loss: 3.5207e-04 - val_dense_68_loss: 4.6714e-04\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 1s 55us/step - loss: 0.0092 - dense_67_loss: 0.0034 - dense_68_loss: 0.0058 - val_loss: 6.2607e-04 - val_dense_67_loss: 2.8989e-04 - val_dense_68_loss: 3.3617e-04\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 0.0080 - dense_67_loss: 0.0035 - dense_68_loss: 0.0045 - val_loss: 6.7629e-04 - val_dense_67_loss: 3.8513e-04 - val_dense_68_loss: 2.9116e-04\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 0.0068 - dense_67_loss: 0.0026 - dense_68_loss: 0.0042 - val_loss: 7.5300e-04 - val_dense_67_loss: 2.8952e-04 - val_dense_68_loss: 4.6348e-04\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 1s 56us/step - loss: 0.0072 - dense_67_loss: 0.0030 - dense_68_loss: 0.0041 - val_loss: 4.5399e-04 - val_dense_67_loss: 2.6938e-04 - val_dense_68_loss: 1.8461e-04\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 0.0083 - dense_67_loss: 0.0029 - dense_68_loss: 0.0054 - val_loss: 6.8665e-04 - val_dense_67_loss: 3.2969e-04 - val_dense_68_loss: 3.5696e-04\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.0056 - dense_67_loss: 0.0021 - dense_68_loss: 0.0035 - val_loss: 5.3391e-04 - val_dense_67_loss: 2.8305e-04 - val_dense_68_loss: 2.5086e-04\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 0s 49us/step - loss: 0.0072 - dense_67_loss: 0.0042 - dense_68_loss: 0.0030 - val_loss: 4.8239e-04 - val_dense_67_loss: 2.5691e-04 - val_dense_68_loss: 2.2549e-04\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 1s 51us/step - loss: 0.0070 - dense_67_loss: 0.0049 - dense_68_loss: 0.0021 - val_loss: 5.0572e-04 - val_dense_67_loss: 2.8398e-04 - val_dense_68_loss: 2.2173e-04\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.0089 - dense_67_loss: 0.0028 - dense_68_loss: 0.0061 - val_loss: 5.1985e-04 - val_dense_67_loss: 2.6040e-04 - val_dense_68_loss: 2.5944e-04\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 1s 71us/step - loss: 0.0077 - dense_67_loss: 0.0030 - dense_68_loss: 0.0047 - val_loss: 4.2583e-04 - val_dense_67_loss: 2.5317e-04 - val_dense_68_loss: 1.7265e-04\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 1s 70us/step - loss: 0.0083 - dense_67_loss: 0.0049 - dense_68_loss: 0.0034 - val_loss: 0.0012 - val_dense_67_loss: 8.6981e-04 - val_dense_68_loss: 2.8483e-04\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 1s 54us/step - loss: 0.0070 - dense_67_loss: 0.0034 - dense_68_loss: 0.0036 - val_loss: 5.3294e-04 - val_dense_67_loss: 2.3830e-04 - val_dense_68_loss: 2.9464e-04\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 1s 56us/step - loss: 0.0070 - dense_67_loss: 0.0033 - dense_68_loss: 0.0037 - val_loss: 5.8627e-04 - val_dense_67_loss: 2.9233e-04 - val_dense_68_loss: 2.9394e-04\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.0066 - dense_67_loss: 0.0024 - dense_68_loss: 0.0042 - val_loss: 4.6639e-04 - val_dense_67_loss: 2.1519e-04 - val_dense_68_loss: 2.5120e-04\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 1s 61us/step - loss: 0.0089 - dense_67_loss: 0.0049 - dense_68_loss: 0.0040 - val_loss: 4.8229e-04 - val_dense_67_loss: 2.5362e-04 - val_dense_68_loss: 2.2867e-04\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 1s 54us/step - loss: 0.0084 - dense_67_loss: 0.0035 - dense_68_loss: 0.0049 - val_loss: 3.1140e-04 - val_dense_67_loss: 1.7436e-04 - val_dense_68_loss: 1.3704e-04\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 1s 61us/step - loss: 0.0062 - dense_67_loss: 0.0030 - dense_68_loss: 0.0032 - val_loss: 3.7116e-04 - val_dense_67_loss: 2.1006e-04 - val_dense_68_loss: 1.6109e-04\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 1s 54us/step - loss: 0.0087 - dense_67_loss: 0.0048 - dense_68_loss: 0.0040 - val_loss: 4.1596e-04 - val_dense_67_loss: 1.9794e-04 - val_dense_68_loss: 2.1802e-04\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.0096 - dense_67_loss: 0.0044 - dense_68_loss: 0.0052 - val_loss: 4.4249e-04 - val_dense_67_loss: 1.9178e-04 - val_dense_68_loss: 2.5071e-04\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.0093 - dense_67_loss: 0.0030 - dense_68_loss: 0.0063 - val_loss: 6.0880e-04 - val_dense_67_loss: 4.1573e-04 - val_dense_68_loss: 1.9307e-04\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 0.0077 - dense_67_loss: 0.0043 - dense_68_loss: 0.0033 - val_loss: 3.6176e-04 - val_dense_67_loss: 1.7487e-04 - val_dense_68_loss: 1.8689e-04\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 1s 56us/step - loss: 0.0074 - dense_67_loss: 0.0033 - dense_68_loss: 0.0041 - val_loss: 2.7517e-04 - val_dense_67_loss: 1.7243e-04 - val_dense_68_loss: 1.0273e-04\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.0047 - dense_67_loss: 0.0019 - dense_68_loss: 0.0029 - val_loss: 2.8035e-04 - val_dense_67_loss: 1.6775e-04 - val_dense_68_loss: 1.1260e-04\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 1s 56us/step - loss: 0.0103 - dense_67_loss: 0.0064 - dense_68_loss: 0.0039 - val_loss: 2.8189e-04 - val_dense_67_loss: 1.5636e-04 - val_dense_68_loss: 1.2553e-04\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.0056 - dense_67_loss: 0.0028 - dense_68_loss: 0.0028 - val_loss: 3.0519e-04 - val_dense_67_loss: 1.6570e-04 - val_dense_68_loss: 1.3949e-04\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.0068 - dense_67_loss: 0.0035 - dense_68_loss: 0.0034 - val_loss: 2.4490e-04 - val_dense_67_loss: 1.5881e-04 - val_dense_68_loss: 8.6085e-05\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.0054 - dense_67_loss: 0.0017 - dense_68_loss: 0.0037 - val_loss: 3.6412e-04 - val_dense_67_loss: 1.5587e-04 - val_dense_68_loss: 2.0825e-04\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.0064 - dense_67_loss: 0.0029 - dense_68_loss: 0.0035 - val_loss: 2.5276e-04 - val_dense_67_loss: 1.5247e-04 - val_dense_68_loss: 1.0029e-04\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 1s 59us/step - loss: 0.0062 - dense_67_loss: 0.0019 - dense_68_loss: 0.0043 - val_loss: 2.6740e-04 - val_dense_67_loss: 1.4934e-04 - val_dense_68_loss: 1.1806e-04\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 0.0091 - dense_67_loss: 0.0046 - dense_68_loss: 0.0045 - val_loss: 2.3229e-04 - val_dense_67_loss: 1.2855e-04 - val_dense_68_loss: 1.0373e-04\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 0s 47us/step - loss: 0.0094 - dense_67_loss: 0.0067 - dense_68_loss: 0.0028 - val_loss: 2.4874e-04 - val_dense_67_loss: 1.7801e-04 - val_dense_68_loss: 7.0733e-05\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 1s 54us/step - loss: 0.0071 - dense_67_loss: 0.0045 - dense_68_loss: 0.0026 - val_loss: 2.1674e-04 - val_dense_67_loss: 1.3735e-04 - val_dense_68_loss: 7.9387e-05\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 0.0068 - dense_67_loss: 0.0029 - dense_68_loss: 0.0039 - val_loss: 2.3277e-04 - val_dense_67_loss: 1.4673e-04 - val_dense_68_loss: 8.6035e-05\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 1s 55us/step - loss: 0.0043 - dense_67_loss: 0.0022 - dense_68_loss: 0.0021 - val_loss: 2.0020e-04 - val_dense_67_loss: 1.1779e-04 - val_dense_68_loss: 8.2417e-05\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 0.0078 - dense_67_loss: 0.0031 - dense_68_loss: 0.0048 - val_loss: 2.1486e-04 - val_dense_67_loss: 1.3271e-04 - val_dense_68_loss: 8.2153e-05\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 0s 50us/step - loss: 0.0052 - dense_67_loss: 0.0028 - dense_68_loss: 0.0025 - val_loss: 2.2050e-04 - val_dense_67_loss: 1.5107e-04 - val_dense_68_loss: 6.9434e-05\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.0060 - dense_67_loss: 0.0023 - dense_68_loss: 0.0037 - val_loss: 2.6491e-04 - val_dense_67_loss: 1.6767e-04 - val_dense_68_loss: 9.7243e-05\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 1s 65us/step - loss: 0.0037 - dense_67_loss: 0.0020 - dense_68_loss: 0.0017 - val_loss: 1.9749e-04 - val_dense_67_loss: 1.2639e-04 - val_dense_68_loss: 7.1094e-05\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 1s 72us/step - loss: 0.0064 - dense_67_loss: 0.0040 - dense_68_loss: 0.0024 - val_loss: 2.2050e-04 - val_dense_67_loss: 1.1825e-04 - val_dense_68_loss: 1.0225e-04\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 1s 55us/step - loss: 0.0077 - dense_67_loss: 0.0032 - dense_68_loss: 0.0045 - val_loss: 1.9060e-04 - val_dense_67_loss: 1.2026e-04 - val_dense_68_loss: 7.0339e-05\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 1s 69us/step - loss: 0.0085 - dense_67_loss: 0.0036 - dense_68_loss: 0.0049 - val_loss: 2.0291e-04 - val_dense_67_loss: 1.1534e-04 - val_dense_68_loss: 8.7573e-05\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 1s 57us/step - loss: 0.0044 - dense_67_loss: 0.0020 - dense_68_loss: 0.0024 - val_loss: 1.7331e-04 - val_dense_67_loss: 1.1363e-04 - val_dense_68_loss: 5.9682e-05\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 0.0049 - dense_67_loss: 0.0023 - dense_68_loss: 0.0025 - val_loss: 1.8387e-04 - val_dense_67_loss: 1.2447e-04 - val_dense_68_loss: 5.9409e-05\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.0034 - dense_67_loss: 0.0021 - dense_68_loss: 0.0013 - val_loss: 1.9391e-04 - val_dense_67_loss: 1.0830e-04 - val_dense_68_loss: 8.5610e-05\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.0062 - dense_67_loss: 0.0031 - dense_68_loss: 0.0031 - val_loss: 2.1724e-04 - val_dense_67_loss: 1.0938e-04 - val_dense_68_loss: 1.0786e-04\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 1s 66us/step - loss: 0.0049 - dense_67_loss: 0.0024 - dense_68_loss: 0.0025 - val_loss: 1.7551e-04 - val_dense_67_loss: 1.0752e-04 - val_dense_68_loss: 6.7994e-05\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.0045 - dense_67_loss: 0.0016 - dense_68_loss: 0.0029 - val_loss: 2.0374e-04 - val_dense_67_loss: 1.0468e-04 - val_dense_68_loss: 9.9061e-05\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.0082 - dense_67_loss: 0.0047 - dense_68_loss: 0.0036 - val_loss: 2.1821e-04 - val_dense_67_loss: 1.0774e-04 - val_dense_68_loss: 1.1047e-04\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 0.0028 - dense_67_loss: 0.0011 - dense_68_loss: 0.0017 - val_loss: 1.5841e-04 - val_dense_67_loss: 1.0036e-04 - val_dense_68_loss: 5.8045e-05\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.0068 - dense_67_loss: 0.0037 - dense_68_loss: 0.0030 - val_loss: 1.5911e-04 - val_dense_67_loss: 1.0063e-04 - val_dense_68_loss: 5.8475e-05\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 1s 64us/step - loss: 0.0051 - dense_67_loss: 0.0023 - dense_68_loss: 0.0028 - val_loss: 1.5832e-04 - val_dense_67_loss: 9.7082e-05 - val_dense_68_loss: 6.1233e-05\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 1s 66us/step - loss: 0.0033 - dense_67_loss: 0.0013 - dense_68_loss: 0.0020 - val_loss: 1.5327e-04 - val_dense_67_loss: 1.0409e-04 - val_dense_68_loss: 4.9183e-05\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.0057 - dense_67_loss: 0.0030 - dense_68_loss: 0.0027 - val_loss: 1.3668e-04 - val_dense_67_loss: 9.4385e-05 - val_dense_68_loss: 4.2290e-05\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 1s 59us/step - loss: 0.0041 - dense_67_loss: 0.0020 - dense_68_loss: 0.0021 - val_loss: 1.3945e-04 - val_dense_67_loss: 1.0083e-04 - val_dense_68_loss: 3.8619e-05\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 1s 55us/step - loss: 0.0063 - dense_67_loss: 0.0030 - dense_68_loss: 0.0033 - val_loss: 1.4612e-04 - val_dense_67_loss: 9.2572e-05 - val_dense_68_loss: 5.3548e-05\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 1s 63us/step - loss: 0.0051 - dense_67_loss: 0.0020 - dense_68_loss: 0.0031 - val_loss: 1.3907e-04 - val_dense_67_loss: 9.4486e-05 - val_dense_68_loss: 4.4581e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 0.0036 - dense_67_loss: 0.0017 - dense_68_loss: 0.0019 - val_loss: 1.3834e-04 - val_dense_67_loss: 8.7447e-05 - val_dense_68_loss: 5.0894e-05\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 0s 47us/step - loss: 0.0019 - dense_67_loss: 7.8851e-04 - dense_68_loss: 0.0011 - val_loss: 1.2873e-04 - val_dense_67_loss: 8.8005e-05 - val_dense_68_loss: 4.0728e-05\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 1s 52us/step - loss: 0.0065 - dense_67_loss: 0.0032 - dense_68_loss: 0.0033 - val_loss: 1.3162e-04 - val_dense_67_loss: 9.1585e-05 - val_dense_68_loss: 4.0031e-05\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 0s 47us/step - loss: 0.0057 - dense_67_loss: 0.0021 - dense_68_loss: 0.0036 - val_loss: 1.8001e-04 - val_dense_67_loss: 9.2968e-05 - val_dense_68_loss: 8.7041e-05\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 1s 53us/step - loss: 0.0068 - dense_67_loss: 0.0052 - dense_68_loss: 0.0016 - val_loss: 1.2970e-04 - val_dense_67_loss: 8.7232e-05 - val_dense_68_loss: 4.2469e-05\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 0.0050 - dense_67_loss: 0.0028 - dense_68_loss: 0.0022 - val_loss: 1.2932e-04 - val_dense_67_loss: 7.7513e-05 - val_dense_68_loss: 5.1811e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x180c2f160f0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(training_data_a.shape)\n",
    "# print(keras.backend.shape(decoded_out))\n",
    "print(autoencoder.summary())\n",
    "# list_of_arrays = []\n",
    "# list_of_arrays.append(np.ndarray(training_data_a))\n",
    "# list_of_arrays.append(np.ndarray(training_data_b))\n",
    "# print(list_of_arrays)\n",
    "autoencoder.fit(x = [training_data_a, training_data_b],y = [training_data_a, training_data_b], epochs=100, batch_size=50, shuffle=True,validation_data=([test_data_a, test_data_b], [test_data_a, test_data_b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred1 = encoder1.predict(test_data_a)\n",
    "pred2 = encoder2.predict(test_data_b)\n",
    "\n",
    "print(pred1.shape)\n",
    "\n",
    "plt.scatter(pred1[:, 0], pred1[:, 1])\n",
    "plt.scatter(pred2[:, 0], pred2[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
