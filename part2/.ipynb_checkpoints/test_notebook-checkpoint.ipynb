{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "###############################################################################\n",
    "##Defining the necessities\n",
    "#Defining (n, k)\n",
    "num_channels = 2\n",
    "num_messages = 4\n",
    "num_bits = 2\n",
    "################################################################################\n",
    "##Generating the training data, one hot vectors of width num_messages\n",
    "num_samples = 10000\n",
    "training_data = np.zeros([num_samples, num_messages])\n",
    "\n",
    "file = open(\"training_data-a.npy\", 'wb')\n",
    "file1 = open(\"training_data-b.npy\", 'wb')\n",
    "\n",
    "for iter in range(num_samples):\n",
    "\n",
    "    training_data[iter, np.random.randint(num_messages)] = 1\n",
    "np.save(file, training_data)\n",
    "print(training_data)\n",
    "\n",
    "training_data = np.zeros([num_samples, num_messages])\n",
    "\n",
    "for iter in range(num_samples):\n",
    "    training_data[iter, np.random.randint(num_messages)] = 1\n",
    "\n",
    "np.save(file1, training_data)\n",
    "print(training_data)\n",
    "file.close()\n",
    "file1.close()\n",
    "###############################################################################\n",
    "##Generating the test data, one hot vectors of width num_messages\n",
    "num_samples = 5000\n",
    "test_data = np.zeros([num_samples, num_messages])\n",
    "\n",
    "file = open(\"test_data-a.npy\", 'wb')\n",
    "file1 = open(\"test_data-b.npy\", 'wb')\n",
    "\n",
    "for iter in range(num_samples):\n",
    "    test_data[iter, np.random.randint(num_messages)] = 1\n",
    "\n",
    "np.save(file, test_data)\n",
    "print(test_data)\n",
    "\n",
    "test_data = np.zeros([num_samples, num_messages])\n",
    "\n",
    "for iter in range(num_samples):\n",
    "    test_data[iter, np.random.randint(num_messages)] = 1\n",
    "\n",
    "np.save(file1, test_data)\n",
    "\n",
    "file.close()\n",
    "file1.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 8)\n",
      "(5000, 8)\n"
     ]
    }
   ],
   "source": [
    "file_train = open(\"training_data-a.npy\", \"rb\")\n",
    "file_train1 = open(\"training_data-b.npy\", \"rb\")\n",
    "file_test = open(\"test_data-a.npy\", \"rb\")\n",
    "file_test1 = open(\"test_data-b.npy\", \"rb\")\n",
    "\n",
    "training_data_a = np.load(file_train)\n",
    "test_data_a = np.load(file_test)\n",
    "training_data_b = np.load(file_train1)\n",
    "test_data_b = np.load(file_test1)\n",
    "\n",
    "training_data = np.concatenate((training_data_a, training_data_b), axis = 1)\n",
    "test_data  = np.concatenate((test_data_a, test_data_b), axis = 1)\n",
    "\n",
    "file_train.close()\n",
    "file_test.close()\n",
    "file_train1.close()\n",
    "file_test1.close()\n",
    "\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midlayer_dim : 4\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "msg_len = 4\n",
    "input_dim = 2*msg_len\n",
    "channel_num = 2\n",
    "encoding_dim = 2*channel_num\n",
    "midlayer_dim = int((msg_len+channel_num)/2)+1\n",
    "print (\"midlayer_dim :\", midlayer_dim )\n",
    "energy_per_bit1 = 5\n",
    "energy_per_bit2 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_msg = Input(shape = (input_dim, ))\n",
    "# input_msg2 = Input(shape = (msg_len, ))\n",
    "\n",
    "\n",
    "lambda_l1_1 = keras.layers.Lambda(lambda x : x[0:msg_len, :])(input_msg)\n",
    "encoded_l1_1 = Dense(msg_len, activation = 'relu')(lambda_l1_1)\n",
    "encoded_l1_2 = Dense(channel_num, activation = 'linear')(encoded_l1_1)\n",
    "encoded_l1_3 = keras.layers.BatchNormalization(axis=1)(encoded_l1_2)\n",
    "\n",
    "lambda_l2_1 = keras.layers.Lambda(lambda x : x[msg_len:2*msg_len, :])(input_msg)\n",
    "encoded_l2_1 = Dense(msg_len, activation = 'relu')(lambda_l2_1)\n",
    "encoded_l2_2 = Dense(channel_num, activation = 'linear')(encoded_l2_1)\n",
    "encoded_l2_3 = keras.layers.BatchNormalization(axis=1)(encoded_l2_2)\n",
    "\n",
    "added_layer = keras.layers.Add()([encoded_l1_3, encoded_l2_3])\n",
    "encoded_l1_4 = keras.layers.GaussianNoise(np.sqrt(channel_num/(2*msg_len*energy_per_bit1)))(added_layer)\n",
    "encoded_l2_4 = keras.layers.GaussianNoise(np.sqrt(channel_num/(2*msg_len*energy_per_bit2)))(added_layer)\n",
    "\n",
    "decoded_l1_1 = Dense(midlayer_dim, activation = 'relu')(encoded_l1_4)\n",
    "decoded_l2_1 = Dense(midlayer_dim, activation = 'relu')(encoded_l2_4)\n",
    "\n",
    "decoded_l1_2 = Dense(midlayer_dim, activation = 'softmax')(decoded_l1_1)\n",
    "decoded_l2_2 = Dense(midlayer_dim, activation = 'softmax')(decoded_l2_1)\n",
    "\n",
    "concat_layer = keras.layers.concatenate([decoded_l1_2, decoded_l2_2])\n",
    "\n",
    "decoded_out = Dense(input_dim)(concat_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate)\n",
    "autoencoder = Model(inputs = input_msg, outputs = [decoded_l1_2, decoded_l2_2])\n",
    "autoencoder.compile(optimizer = adam, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder1 = Model(input_msg, encoded_l1_3)\n",
    "encoder2 = Model(input_msg, encoded_l2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 2 arrays: [array([[0., 0., 1., 0.],\n       [1., 0., 0., 0.],\n       [0., 0., 1., 0.],\n       ...,\n       [0., 0., 0., 1.],\n       [0., 0., 0., 1.],\n       [0., 1., 0., 0.]]), array([[0., 0., 0., 1.],\n       [0....",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-146ccafb58dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtraining_data_a\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_data_b\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtraining_data_a\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mtraining_data_b\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;34m'Expected to see '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' array(s), '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;34m'but instead got the following list of '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 2 arrays: [array([[0., 0., 1., 0.],\n       [1., 0., 0., 0.],\n       [0., 0., 1., 0.],\n       ...,\n       [0., 0., 0., 1.],\n       [0., 0., 0., 1.],\n       [0., 1., 0., 0.]]), array([[0., 0., 0., 1.],\n       [0...."
     ]
    }
   ],
   "source": [
    "autoencoder.fit(training_data,training_data, epochs=100, batch_size=50, shuffle=True, validation_data=(test_data, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
